DOCKER 

docker is used for thee reasons :
1. Kubernetes / Container orchestration
2. Running processes in isolated enviornments 
3. Starting projects/ auxilary serices

containers are a way to package and distribute software applications which makes it easy to deploy consistently across different OS

1. install docker 
2. in the terminal write docker -> if runs docker is installed 

DOCKER ARCHITECTURE 

Docker CLI -> Docker Engine -> OS Kernel -> Containers
Docker Engine <-> Docker Registry (for pulling/pushing images)

1. Docker Architecture

Docker uses a client-server architecture. It allows you to package applications and dependencies into containers that can run anywhere. The main components of Docker architecture are:

Components:

Docker Client (CLI)
The interface through which users interact with Docker.
You run commands like docker run, docker build, docker push, etc.
The CLI communicates with the Docker Engine via a REST API.
Docker Engine (Server/Daemon)
The core of Docker, responsible for building, running, and managing containers.
Runs as a background process (dockerd) on your host system.
Manages images, containers, networks, and volumes.

Docker Registry
A repository to store Docker images.
Example: Docker Hub (public) or private registries.
Images are pulled from or pushed to registries.

Docker Objects
Images: Read-only templates to create containers.
Containers: Running instances of images.
Volumes: For persistent storage.
Networks: For communication between containers.

2. Docker Engine

The Docker Engine is the heart of Docker. It’s responsible for:
Building images: Using Dockerfile instructions to create container images.
Running containers: Launching isolated environments using images.
Managing containers: Start, stop, remove, and monitor containers.
Networking: Manage communication between containers.
Storage: Manage persistent data using volumes.
Components of Docker Engine:
Docker Daemon (dockerd)
Listens for Docker API requests from the CLI or other tools.
Manages images, containers, networks, and storage.
REST API
The interface for communication between the CLI and daemon.
Container Runtime
Runs containers using OS-level virtualization (cgroups, namespaces).
By default, Docker uses runc as its runtime.

3. Docker CLI

Docker Command Line Interface is how users interact with Docker.
Commands are sent to the Docker Engine, which executes them.

Examples:
docker build → Builds an image from a Dockerfile
docker run → Runs a container from an image
docker ps → Lists running containers
docker pull / docker push → Pulls or pushes images to registry
How it works:
User types a command in CLI.
CLI sends a REST API request to Docker Engine.
Docker Engine executes the request and returns the result.

4. Docker Registry

Docker Registry is a storage and distribution system for Docker images.
Public Registry: Docker Hub
Private Registry: Hosted on private servers for internal use

Workflow:

Push an image:
docker push myregistry/myimage:tag → sends image to registry
Pull an image:
docker pull myregistry/myimage:tag → downloads image from registry
Versioning:
Each image can have multiple tags, representing versions.

hub.docekr.com -> contains all the images


IMAGES vs CONTAINERS 

1. Docker Images

Definition: A Docker image is a read-only template used to create containers.
Purpose: Stores everything needed to run an application: code, libraries, dependencies, and environment variables.
Static: Cannot be changed once created (immutable).
Analogy: Like a blueprint or recipe for a cake.
Example: python:3.11 is an image containing Python 3.11 and its environment.

2. Docker Containers

Definition: A container is a running instance of an image.
Purpose: Executes the application in an isolated environment.
Dynamic: Can be started, stopped, modified, or deleted.
Analogy: Like the actual cake baked from the recipe.
Example: Running docker run -it python:3.11 creates a container from the Python image.

docker run -p 27017 : 27017 mongo 

docker run → Starts a new container from an image.
mongo → The image name (official MongoDB image).
-p 27017:27017 → This is port mapping (host:container).

COMMON COMMANDS 

- docker images -> to check the images that are running 
- docker run [image name] -> to run the image 
- docker rmi [image name] -> to remove the image
- docker ps -> what all containers running
- docker build -> building our own images 
- docker push -> pushing images to hub.docker.com 
- docker kill [container id] -> kill the instance
- docker exec -it [containerid] sh -> getting inside the container in shell access

DOCKER FILE 

if you want to containerize a project then you have to create a docker file for it 

create a new node project -> node_project
create a dockerfile in it 
write the commands in teh docker file 
create a .dockerignore file and write the ignored files in that 
run the build command 
- docker build -t hello-app . 

now this image is present on your docker to run the image 
- docker run -p 3000:3000 hello-app

to inject the env vaiables you have to use -e 
- docker run -p 3000:3000 -e DATABASE_URL=jfajfuiafdjhkasgdyusadu hello-app

now you can push the image on docker hub

docker login 
docker push stealthsilver/hello-app


--------------------------------------------------------------------------------------------------------

Layers in docker 

layers are a fundamnetal part of the image architecture that allows docker to be efficient, fast and portable
when we create an image we are layering over a base image

in the dockerfile has 4 layers 

base layer -> workdir image -> copy the source code -> npm install 

layers are required for caching 
this makes the project reusable and sharable 

the base layer is cached also all the subsequent layers are also cached
when you docker build then the layers are cached

if we change the code then only the third and forth layer will re build as others are cached 

Optimising the docker file

change the docker file in a way then npm install must be cached as it takes a lot of time while building

COPY ./package.json ./package.json 
COPY ./package-lock.json ./package-lock.json 
RUN npm install 

pushing the npm install upwards so that it is cached even when the code changes. only when the package.json changes then npm install will run

NETWORKS AND VOLUMES 

if we restart a container then the data goes away
docker containers are stateless so that they dont store data 

if we want the data to be persisted when the docker restarts

for mongo the data is stored in data/db we need to store it somewhere 

for the docker container a volume is connected and then the persistant files are stored in the volume
when the docker stops the data gets stored in the volume and when the docker re-runs then the data is used from the volume

- docker volume create mongo_db_data 
- docker run -p 27017:27017 -v mongo_db_data:/data/db mongo

NETWORKS 

two docker containers cannot communicate with each other. 
we use a network to connect the two conatiners 

creating a newtwork 

- docker network create [name]